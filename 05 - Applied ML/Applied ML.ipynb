{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal stack of pandas, numpy, matplotlib and seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical test library\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "original = pd.read_csv(\"CrowdstormingDataJuly1st.csv\", parse_dates=['birthday'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#scatter_matrix(original, figsize=(30, 30), diagonal='histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First glimpse at data content\n",
    "original.ix[:10,:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges in dataset:\n",
    "\n",
    "- Skin ratings don't match -> take average\n",
    "- Picture was missing -> exclude from training\n",
    "- No cards drawn in some dyads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prune dataset where there is no rater information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = original[['refNum', 'games']].groupby(['refNum']).sum()\n",
    "grouped_df = grouped[grouped['games'] >= 22].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "referees_df = original[original['refNum'].isin(grouped_df['refNum'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to round on quater ratings\n",
    "def round_quarter(x):\n",
    "    return round(x*4)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater = referees_df.copy()\n",
    "rater['rater_mean'] = round_quarter(rater[['rater1','rater2']].mean(axis=1))\n",
    "rater['rater_mean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater = rater.dropna(subset=['rater_mean'])\n",
    "rater['rater_mean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of unusable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rater.columns[:16] | rater.columns[20:] - ['Alpha_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = rater\n",
    "\n",
    "# Take only the features that describes the player\n",
    "# Feature 1 contains the short name of the player\n",
    "# Feature 16 and upwards contains the information about the rater\n",
    "\n",
    "#TODO: add referee columns\n",
    "\n",
    "# features = rater[ rater.columns[:16] | ['rater_mean']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string values to floats by LabelEncoder to make them readable by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features['refNum'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split birthday into year...\n",
    "# features_t = features[features['birthday'].str.split('-')]\n",
    "# features_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Workaround by removing problematic columns\n",
    "#features = features.drop('position', axis=1)\n",
    "#features = features.drop('Alpha_3', axis=1)\n",
    "#features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "When we have categorical data, we need to transform them so that they can be taken into account in our model. One way of doing this is to use LabelEncoder and OneHotEncoder from SciKitLearn. \n",
    "\n",
    "LabelEncoder converts each category into an integer, so that we don't have to deal with strings. After doing this transformation, we use OneHotEncoder to make a binary feature for each category. This way, we can capture for instance wether a person has played for both Fulham FC and Manchester City. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We select those features that contains some categorical value\n",
    "# At the same time, we fill the NaN-values by '' to avoid problems later on\n",
    "categorical_features = features[['leagueCountry','position','club']].fillna('Missing')\n",
    "\n",
    "\n",
    "columns = []\n",
    "for cat in categorical_features.columns:\n",
    "    counts = categorical_features[cat].value_counts()\n",
    "    n_cats = len(counts)\n",
    "    \n",
    "    columns += list(counts.keys().values)\n",
    "\n",
    "    print(\"%s (%i categories):\" % (cat.capitalize(), n_cats))\n",
    "    print(\"%s\\n\" % counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our categorical_features now looks like this:\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Transform each category into an integer\n",
    "## For this, we use LabelEncoder from sklearn\n",
    "\n",
    "label_encoded = categorical_features.apply(LabelEncoder().fit_transform)\n",
    "label_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Transform each integer into a binary feature\n",
    "## For this, we use OneHotEncoder\n",
    "\n",
    "one_hot_encoded = OneHotEncoder().fit_transform(label_encoded).toarray()\n",
    "\n",
    "one_hot_encoded_features = pd.DataFrame(one_hot_encoded, columns=columns)\n",
    "\n",
    "one_hot_encoded_features.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the original categorical features, and attatch the new one_hot_encoded ones :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_no_categorical = features.drop( categorical_features.columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_one_hot_encoded = features_no_categorical.join(one_hot_encoded_features)\n",
    "features_one_hot_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating over the players:\n",
    "\n",
    "We now run grouping and aggregation of our dataframe. The aggregation functions used are defined in two dictionarys.\n",
    "Each element of the dictionarys contains of a column name and an aggregation function, which is applied to our grouped features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_features = features_one_hot_encoded.groupby(['playerShort', 'birthday', 'height', 'weight'])\n",
    "\n",
    "# Aggregating with max gives us a logical OR on the one hot encoded features\n",
    "one_hot_enc_aggfunc_mapping = {i: max for i in one_hot_encoded_features.columns}\n",
    "column_aggfunc_mapping = {'rater_mean': max,'games': sum, 'victories': sum, 'ties': sum, 'defeats': sum, 'goals': sum, 'yellowCards': sum, 'yellowReds': sum, 'redCards': sum, 'meanIAT': np.mean, 'nIAT': np.mean, 'seIAT': np.mean, 'meanExp': np.mean, 'nExp': np.mean, 'seExp': np.mean}\n",
    "\n",
    "aggregated_features = grouped_features.agg({**one_hot_enc_aggfunc_mapping, **column_aggfunc_mapping})\n",
    "aggregated_features = aggregated_features.reset_index().set_index('playerShort')\n",
    "aggregated_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared dataset for further processing with ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X_lab = aggregated_features.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "Xy = X_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = Xy.drop(['rater_mean'], axis=1)\n",
    "Y_train = Xy['rater_mean']\n",
    "#Y_train = np.asarray(rater['rater_mean'], dtype=\"|S6\")\n",
    "X_train = X_train.tail(-3)\n",
    "Y_train = Y_train[3:]\n",
    "print(type(Y_train))\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print('FEATURES')\n",
    "print(X_train.head(10))\n",
    "print('LABELS')\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning by RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(verbose=1)\n",
    "print(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import test data set to test classifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test prediction\n",
    "X_pred = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "rfc.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use classifier with live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plug of unplug live data\n",
    "X = X_train\n",
    "y = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual prediction test\n",
    "#X_pred = features.head(3)\n",
    "#rfc.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect most relevant features of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance assessment through cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "scores = cross_val_score(rfc, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print('Achieved model score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize score results as boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Verify if classifier should be categorical of binary\n",
    "- Take into account the referee statistics in a smart way (see Slack discussion)\n",
    "- Try different parameters for the random forest to compare results (bias, variance)\n",
    "- Bonus\n",
    "- Exercise 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
