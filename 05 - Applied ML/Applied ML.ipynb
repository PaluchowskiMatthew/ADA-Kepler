{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal stack of pandas, numpy, matplotlib and seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical test library\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "print(sklearn.__file__)\n",
    "print(sklearn.__version__)\n",
    "print(sklearn.__path__)\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(10)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "original = pd.read_csv(\"CrowdstormingDataJuly1st.csv\", parse_dates=['birthday'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First glimpse at data content\n",
    "original.ix[:5,:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original.ix[:5,13:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges in dataset:\n",
    "\n",
    "- Skin ratings don't match -> take average\n",
    "- Picture was missing -> exclude from training\n",
    "- No cards drawn in some dyads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prune dataset where there is no rater information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to see the efect when we drop rows\n",
    "def dropping_stats(df):\n",
    "    drop_perc =  100 * (original.shape[0] - df.shape[0]) / original.shape[0]\n",
    "    print(\"%.2f%% of original data droped.\" % (drop_perc) )\n",
    "    \n",
    "    print(\"Now: %d rows\" % df.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns that will not be relevant for our model\n",
    "original['year'] = original['birthday'].apply(lambda x: x.year)\n",
    "\n",
    "#nan-dropping\n",
    "data = original.dropna(how='any', subset=['rater1', 'rater2', 'meanExp', 'meanIAT'])\n",
    "data.drop(['photoID', 'refCountry', 'Alpha_3', 'player', 'birthday'], errors='raise', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If a referee is present in less than 22 triads (rows), he cannot have\n",
    "# refereed a match in these leages.\n",
    "ref_grouped = data[['refNum', 'games']].groupby(['refNum']).sum()\n",
    "ref_filtered = ref_grouped[ref_grouped['games'] >= 22].reset_index()\n",
    "\n",
    "# Therefore, we filter the data on this condition\n",
    "has_referee = data[data['refNum'].isin(ref_filtered['refNum'].values)]\n",
    "cleaned = has_referee\n",
    "dropping_stats(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned['meanIAT'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with nan-values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned['weight'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned['height'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned['leagueCountry'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fill these with the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned['weight'].fillna(cleaned['weight'].mean(), inplace=True)\n",
    "cleaned['height'].fillna(cleaned['height'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cleaned['height'].hasnans)\n",
    "print(cleaned['weight'].hasnans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the class feature\n",
    "\n",
    "To make the class of the skin color of each player, we take the mean of the value from the two raters. \n",
    "Players that does not have a rating gets dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_binary_class(x):\n",
    "    \"\"\" Returns 0 for players rated below 0.5 ('light-skinned') and 1 for players rated above ('dark-skinned')\"\"\"\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the mean of the two raters value\n",
    "mean_rating = has_referee[['rater1', 'rater2']].mean(axis=1)\n",
    "\n",
    "# Drop the players that does not have a rating\n",
    "mean_rating.dropna(inplace=True)\n",
    "\n",
    "dropping_stats(mean_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The players now have a rating between 0 and 1, real numbers\n",
    "mean_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since we want to do a binary classification, we convert the real numbers to 0/1\n",
    "\n",
    "binary_class = mean_rating.apply(get_binary_class)\n",
    "binary_class.name = 'class'\n",
    "binary_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only the data rows where we have the class\n",
    "\n",
    "has_class = has_referee.ix[binary_class.index]\n",
    "dropping_stats(has_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = binary_class.value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"%.2f%% of the examples are light skinned\" % (counts[0] / (counts[0] + counts[1]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string values to floats by LabelEncoder to make them readable by the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "When we have categorical data, we need to transform them so that they can be taken into account in our model. One way of doing this is to use LabelEncoder and OneHotEncoder from SciKitLearn. \n",
    "\n",
    "LabelEncoder converts each category into an integer, so that we don't have to deal with strings. After doing this transformation, we use OneHotEncoder to make a binary feature for each category. This way, we can capture for instance wether a person has played for both Fulham FC and Manchester City. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_encoded = pd.get_dummies(has_class['leagueCountry'])\n",
    "countries_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "position_encoded = pd.get_dummies(has_class['position'])\n",
    "position_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the original categorical features, and attatch the new one_hot_encoded ones :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our hypothesis is that the combination of the referees 'discrimination score' and cards given,\n",
    "# might help us classify the players. E.g., if a player got many cards from racist referees, \n",
    "# he is more likely dark-skinned.\n",
    "\n",
    "red_exp = has_class['redCards'] * has_class['meanExp'] \n",
    "yellow_exp = has_class['yellowCards'] * has_class['meanExp']\n",
    "yellow_red_exp = has_class['yellowReds'] * has_class['meanExp']\n",
    "\n",
    "red_iat = has_class['redCards'] * has_class['meanIAT'] \n",
    "yellow_iat = has_class['yellowCards'] * has_class['meanIAT']\n",
    "yellow_red_iat = has_class['yellowReds'] * has_class['meanIAT']\n",
    "\n",
    "cards_iat = pd.concat([red_exp, yellow_exp, yellow_red_exp, red_iat, yellow_iat, yellow_red_iat], axis=1)\n",
    "cards_iat.columns = ['red_exp', 'yellow_exp', 'red_yellow_exp', 'red_iat', 'yellow_iat', 'red_yellow_iat']\n",
    "cards_iat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_from_orig = ['playerShort', 'year', 'height', 'weight', 'games', 'victories','ties', 'defeats','goals','yellowCards',\n",
    "                     'yellowReds','redCards','meanIAT', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'seExp']\n",
    "\n",
    "colomns_one_hot_enoded = countries_encoded.columns | position_encoded.columns\n",
    "\n",
    "features = has_class[columns_from_orig].join(countries_encoded).join(position_encoded).join(cards_iat).join(binary_class)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating over the players:\n",
    "\n",
    "We now run grouping and aggregation of our dataframe. The aggregation functions used are defined in two dictionarys.\n",
    "Each element of the dictionarys contains of a column name and an aggregation function, which is applied to our grouped features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players = features.groupby(['playerShort','year','height', 'weight'])\n",
    "\n",
    "# We sum over the one hot encoded features\n",
    "one_hot_aggregation = {i: max for i in colomns_one_hot_enoded}\n",
    "\n",
    "# And then sum over games, victories, ties, defeats, goals, cards, \n",
    "column_aggfunc_mapping = {'class': max, 'games': sum, 'victories': sum, 'ties': sum, 'defeats': sum, 'goals': sum,\n",
    "                          'yellowCards': sum, 'yellowReds': sum, 'redCards': sum, 'red_exp': sum,\n",
    "                          'yellow_exp': sum, 'red_yellow_exp': sum, 'red_iat': sum,\n",
    "                          'yellow_iat': sum, 'red_yellow_iat': sum, 'meanIAT': np.mean, 'meanExp': np.mean}\n",
    "\n",
    "# Union the aggregation function dicts\n",
    "agg_funcs = {**one_hot_aggregation, **column_aggfunc_mapping}\n",
    "\n",
    "agg_features = players.agg(agg_funcs)\n",
    "agg_features = agg_features.reset_index().set_index('playerShort')\n",
    "agg_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = preprocessing.normalize(agg_features, norm='l2')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning by RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = agg_features.drop('class', axis=1)\n",
    "y = agg_features['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.value_counts().plot(kind='barh', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('There are about %.2f%% 0s in the class vector.' % (y.value_counts()[0] / y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that by allways predicting 0, we could achieve an accuracy of approximately 60%. \n",
    "We should therefore expect that our classifier performs at least as good as this, and hopefully significantly better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=10, random_state=42) \n",
    "\n",
    "param_grid = {\n",
    "    'max_features': ['log2','sqrt', None],\n",
    "    'min_samples_leaf': [2,20,200],\n",
    "    'max_depth': [4,8,16,None],\n",
    "}\n",
    "\n",
    "#(8)    'max_depth': [4,8,16,None],\n",
    "#(gini) 'criterion': ['gini', 'entropy'],\n",
    "#(log2) 'max_features': ['log2','sqrt', None],\n",
    "#(20) 'min_samples_split': [2,20,200],\n",
    "#(2) 'min_samples_leaf': [2,20,200],\n",
    "#(20) 'min_samples_split': [2,20,200],\n",
    "#(1e-7) 'min_impurity_split': [1e-07, 1e-06, 1e-05],\n",
    "#(True) 'bootstrap': [True, False],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing for F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, scoring='f1', param_grid=param_grid, cv=10, verbose=True, n_jobs=-1)\n",
    "CV_rfc.fit(X, y)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, scoring='accuracy', param_grid=param_grid, cv=10, verbose=True, n_jobs=-1)\n",
    "CV_rfc.fit(X, y, )\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We use the results found in the grid search to tune our random forrest classifier. \n",
    "\n",
    "Description of the hyperparameters:\n",
    "\n",
    "- n_estimators: The number of trees used in random forest\n",
    "- min_samples_leaf: TODO\n",
    "- max_features: TODO\n",
    "- max_depth: The maximal depth of the tree\n",
    "\n",
    "- bootstrap: Todo\n",
    "- oob_score: Todo\n",
    "- n_jobs: Number of processes used in the calculation. -1 uses all avilable.\n",
    "- random_state: Seed for the random generator, to give reproducable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, max_features='log2', max_depth=None, random_state=4, n_jobs=-1)\n",
    "print(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit our model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect most relevant features of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d : %s  (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance assessment \n",
    "\n",
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "scores = cross_val_score(rfc, X, y, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "print(scores)\n",
    "print('Achieved model score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize score results as boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_helpers import custom_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = y\n",
    "y_pred = rfc.predict(X)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "custom_confusion_matrix(cm, classes=['white','black'], title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_confusion_matrix(cm, classes=['white','black'], normalize=True, title='Confusion matrix, with normalized values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance of result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the distribution of the achieved learning score against the learning score value achieved by a simple DummyClassifier to see if our model actually performs significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "dummy_ratio = 0.83\n",
    "t,p_value = ttest_1samp(scores, dummy_ratio, axis=0)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is below 0.05 we don't reject the null hypothesis which is the assumption that the mean of the random sample consisting of the results of our crossvalidation is equal to true mean, consisting of the dummy classifier ratio. This concludes that our models performance is not significantly better than the datasets random level class ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_sizes = np.arange(300,1201, int((1201-300)/20))\n",
    "train_sizes = np.arange(0.1, 1.0, 0.8/3)\n",
    "train_sizes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=1000, min_samples_leaf=2, max_features='log2', max_depth=8,\n",
    "    bootstrap=True, oob_score=True, random_state=4, n_jobs=-1)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "     rfc, X, y, train_sizes=train_sizes, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Learning curve\")\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, '-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, '-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Club skin-color rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: is this cheating?\n",
    "club_class = pd.concat([has_class['club'], binary_class], axis=1)\n",
    "club_class_mean = club_class.groupby('club').mean()\n",
    "club_class_mean.columns = ['club_class_mean']\n",
    "club_class_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "club_stats = pd.DataFrame(has_class['club']).merge(club_class_mean, left_on='club', right_index=True, how='left')\n",
    "club_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.savefig(\"Learning_curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal stack of pandas, numpy, matplotlib and seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "import plot_helpers\n",
    "\n",
    "# Statistical test library\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "original = pd.read_csv(\"CrowdstormingDataJuly1st.csv\", parse_dates=['birthday'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#scatter_matrix(original, figsize=(30, 30), diagonal='histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First glimpse at data content\n",
    "original.ix[:10,:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges in dataset:\n",
    "\n",
    "- Skin ratings don't match -> take average\n",
    "- Picture was missing -> exclude from training\n",
    "- No cards drawn in some dyads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prune dataset where there is no rater information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = original[['refNum', 'games']].groupby(['refNum']).sum()\n",
    "grouped_df = grouped[grouped['games'] >= 22].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "referees_df = original[original['refNum'].isin(grouped_df['refNum'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to round on quater ratings\n",
    "def round_quarter(x):\n",
    "    return round(x*4)/4\n",
    "\n",
    "def binary_class(x):\n",
    "    return round(x*2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater = referees_df.copy()\n",
    "#rater['rater_mean'] = round_quarter(rater[['rater1','rater2']].mean(axis=1))\n",
    "rater['rater_mean'] = binary_class(rater[['rater1','rater2']].mean(axis=1))\n",
    "rater['rater_mean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater = rater.dropna(subset=['rater_mean'])\n",
    "rater['rater_mean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of unusable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rater.columns[:16] | rater.columns[20:] - ['Alpha_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rater.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = rater\n",
    "\n",
    "# Take only the features that describes the player\n",
    "# Feature 1 contains the short name of the player\n",
    "# Feature 16 and upwards contains the information about the rater\n",
    "\n",
    "#TODO: add referee columns\n",
    "features = features.reset_index(drop=True)\n",
    "# features = rater[ rater.columns[:16] | ['rater_mean']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string values to floats by LabelEncoder to make them readable by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features['refNum'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split birthday into year...\n",
    "# features_t = features[features['birthday'].str.split('-')]\n",
    "# features_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Workaround by removing problematic columns\n",
    "#features = features.drop('position', axis=1)\n",
    "#features = features.drop('Alpha_3', axis=1)\n",
    "#features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "When we have categorical data, we need to transform them so that they can be taken into account in our model. One way of doing this is to use LabelEncoder and OneHotEncoder from SciKitLearn. \n",
    "\n",
    "LabelEncoder converts each category into an integer, so that we don't have to deal with strings. After doing this transformation, we use OneHotEncoder to make a binary feature for each category. This way, we can capture for instance wether a person has played for both Fulham FC and Manchester City. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We select those features that contains some categorical value\n",
    "# At the same time, we fill the NaN-values by '' to avoid problems later on\n",
    "categorical_features = features[['leagueCountry','position','club']].fillna('Missing')\n",
    "\n",
    "\n",
    "columns = []\n",
    "for cat in categorical_features.columns:\n",
    "    counts = categorical_features[cat].value_counts()\n",
    "    n_cats = len(counts)\n",
    "    \n",
    "    columns += list(counts.keys().values)\n",
    "\n",
    "    print(\"%s (%i categories):\" % (cat.capitalize(), n_cats))\n",
    "    print(\"%s\\n\" % counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our categorical_features now looks like this:\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Transform each category into an integer\n",
    "## For this, we use LabelEncoder from sklearn\n",
    "\n",
    "label_encoded = categorical_features.apply(LabelEncoder().fit_transform)\n",
    "label_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Transform each integer into a binary feature\n",
    "## For this, we use OneHotEncoder\n",
    "\n",
    "one_hot_encoded = OneHotEncoder().fit_transform(label_encoded).toarray()\n",
    "\n",
    "one_hot_encoded_features = pd.DataFrame(one_hot_encoded, columns=columns)\n",
    "\n",
    "one_hot_encoded_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the original categorical features, and attatch the new one_hot_encoded ones :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_no_categorical = features.drop( categorical_features.columns, axis=1)\n",
    "features_no_categorical.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_one_hot_encoded = features_no_categorical.join(one_hot_encoded_features)\n",
    "features_one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cards = features_one_hot_encoded[['redCards', 'yellowCards', 'yellowReds']]\n",
    "\n",
    "iat = pd.DataFrame(cards.sum(axis=1) * features_one_hot_encoded['meanIAT'], columns=['IAT_comb'])\n",
    "features_one_hot_encoded.join(iat, rsuffix=\"IAT_comb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanIAT = features_one_hot_encoded['meanIAT']\n",
    "combIAT = pd.DataFrame(cards.sum(axis=1) * meanIAT, columns=['combIAT'])\n",
    "\n",
    "features_with_gen = features_one_hot_encoded.join(combIAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating over the players:\n",
    "\n",
    "We now run grouping and aggregation of our dataframe. The aggregation functions used are defined in two dictionarys.\n",
    "Each element of the dictionarys contains of a column name and an aggregation function, which is applied to our grouped features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_features = features_with_gen.groupby(['playerShort', 'birthday', 'height', 'weight'])\n",
    "\n",
    "# Aggregating with max gives us a logical OR on the one hot encoded features\n",
    "one_hot_enc_aggfunc_mapping = {i: max for i in one_hot_encoded_features.columns}\n",
    "column_aggfunc_mapping = {'rater_mean': max,'games': sum, 'victories': sum, 'ties': sum, 'defeats': sum, 'goals': sum, 'yellowCards': sum, 'yellowReds': sum, 'redCards': sum, 'meanIAT': np.mean, 'nIAT': np.mean, 'meanExp': np.mean, 'nExp': np.mean, 'combIAT': sum}\n",
    "\n",
    "aggregated_features = grouped_features.agg({**one_hot_enc_aggfunc_mapping, **column_aggfunc_mapping})\n",
    "aggregated_features = aggregated_features.reset_index().set_index('playerShort')\n",
    "aggregated_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared dataset for further processing with ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X_lab = aggregated_features.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "Xy = X_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = Xy.drop(['rater_mean'], axis=1)\n",
    "Y_train = Xy['rater_mean']\n",
    "#Y_train = np.asarray(rater['rater_mean'], dtype=\"|S6\")\n",
    "X_train = X_train.tail(-3)\n",
    "Y_train = Y_train[3:]\n",
    "print(type(Y_train))\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print('FEATURES')\n",
    "print(X_train.head(10))\n",
    "print('LABELS')\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = preprocessing.normalize(X_train, norm='l2')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning by RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(verbose=1, min_samples_split=2, oob_score=True)\n",
    "print(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import test data set to test classifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test prediction\n",
    "X_pred = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "rfc.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use classifier with live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plug of unplug live data\n",
    "X = X_train\n",
    "y = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual prediction test\n",
    "#X_pred = features.head(3)\n",
    "#rfc.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect most relevant features of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance assessment through cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "scores = cross_val_score(rfc, X, y, cv=50, scoring='accuracy')\n",
    "print(scores)\n",
    "print('Achieved model score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize score results as boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the aggregation the referee information grouping by soccer player we use original features only, since it was not specified in the homework description whether or not we should use our base features alongside engineered ones.\n",
    "\n",
    "Moreover using base features for clustering is a little easier to comprehend and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the referee information grouping by soccer player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Group by soccer player\n",
    "grouped_features = original_features.groupby(['playerShort', 'birthday', 'height', 'weight'])\n",
    "grouped_features.first()\n",
    "\n",
    "#Aggregate referee information\n",
    "column_aggfunc_mapping = {'rater_mean': max,'games': sum, 'victories': sum, 'ties': sum, \\\n",
    "                          'defeats': sum, 'goals': sum, 'yellowCards': sum, 'yellowReds': sum, \\\n",
    "                          'redCards': sum, 'meanIAT': np.mean, 'nIAT': np.mean, 'meanExp': np.mean, \\\n",
    "                          'nExp': np.mean}\n",
    "\n",
    "aggregated_features = grouped_features.agg( column_aggfunc_mapping ) \\\n",
    "                            .reset_index() \\\n",
    "                            .set_index('playerShort')\n",
    "\n",
    "#Convert birthday date column to seperate year, month and day columns\n",
    "aggregated_features['year'] = aggregated_features['birthday'].dt.year\n",
    "aggregated_features['month'] = aggregated_features['birthday'].dt.month\n",
    "aggregated_features['day'] = aggregated_features['birthday'].dt.day\n",
    "\n",
    "aggregated_features = aggregated_features.drop(['birthday'], axis=1)\n",
    "aggregated_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to have only two disjoint clusters lets use kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Y_true = aggregated_features['rater_mean'].round().as_matrix() #We want binary classification / two distinct clusters\n",
    "X = aggregated_features.drop(['rater_mean'], axis=1)\n",
    "X = X.apply(LabelEncoder().fit_transform)\n",
    "X_scaled = preprocessing.scale(X, axis=1)\n",
    "X_copy = X.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=-2, init='k-means++').fit(X_scaled)\n",
    "Y_pred = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, the count of true negatives is C_{0,0}, false negatives is C_{1,0}, true positives is C_{1,1} and false positives is C_{0,1}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline KMeans clustering with 2 clusters and all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "conf_mat = confusion_matrix(Y_true, Y_pred)\n",
    "silhouette = silhouette_score(X_scaled, Y_pred)\n",
    "\n",
    "plot_helpers.custom_confusion_matrix(conf_mat, classes=['White skin', 'Black skin'],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print(\"Silhouette score: \", silhouette)\n",
    "print(\"Accuracy: \", (conf_mat[0,0] + conf_mat[1,1]) / (conf_mat[0,0] + conf_mat[1,1] + conf_mat[0,1] + conf_mat[1,0]))\n",
    "print(\"F1 score: \", f1_score(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to drop features in order which seems to be the most resonable - where we believe two disjoint clusters are easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_copy\n",
    "#Drop all features below = leave 'defeats' feature\n",
    "to_drop = ['day', 'month', 'year', 'height', 'weight', 'goals', 'games', 'ties', 'victories', 'nIAT', 'nExp', 'meanExp', 'yellowReds', 'yellowCards', 'redCards']\n",
    "silhouettes = []\n",
    "for drop_column in to_drop:\n",
    "    \n",
    "    X = X.drop([drop_column], axis=1)\n",
    "    X_scaled =  preprocessing.scale(X, axis=1)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=-2, init='k-means++').fit(X_scaled)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    conf_mat = confusion_matrix(Y_true, Y_pred)\n",
    "    silhouette = silhouette_score(X_scaled, Y_pred)\n",
    "    silhouettes.append(silhouette)\n",
    "    \n",
    "plt.title('Silhouette score after each feature drop')\n",
    "silhouettes_plt = sns.barplot(x=to_drop, y=silhouettes, palette=\"Greens_d\")\n",
    "plt.setp(silhouettes_plt.get_xticklabels(), rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plot_helpers.custom_confusion_matrix(conf_mat, classes=['White skin', 'Black skin'],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print(\"Accuracy: \", (conf_mat[0,0] + conf_mat[1,1]) / (conf_mat[0,0] + conf_mat[1,1] + conf_mat[0,1] + conf_mat[1,0]))\n",
    "print(\"F1 score: \", f1_score(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see creating two distinct clusters with high silhouette score is not a problem if distincitve feature is selected and quite suprisingly it achieves high accuracy. However this is simply due to the fact that majority of the players are white. Moreover it is worth noting that F1 measure/score is very low which means that although two distinct clusters where achieved, they are not race based (whites and blacks are mixed in both clusters, not blacks in one and whites in other). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to check if two distinct clusters for which players with dark and light skin colors belong to different clusters can be achieved, we brute forced calculations to find what is the best feature dropping order for which Silhouette score and F1 score is highest:\n",
    "```\n",
    "n_iters = 100\n",
    "best_silhouette = 0.0\n",
    "best_f1 = 0.0\n",
    "best_drop_order = []\n",
    "to_drop_list = []\n",
    "Y_pred_best\n",
    "for i in range(n_iters):\n",
    " \n",
    "    to_drop = ['day', 'month', 'year', 'height', 'weight', 'goals', 'games','nIAT', 'nExp', 'meanExp','ties', 'yellowReds', 'yellowCards', 'redCards','defeats', 'victories',]\n",
    "    np.random.shuffle(to_drop)\n",
    "    to_drop = to_drop[:-1]\n",
    "    X = X_copy\n",
    "    \n",
    "    print(\"Iteration: \", i)\n",
    "    to_drop_list.append(to_drop)\n",
    "\n",
    "    for drop_column in to_drop:\n",
    "        X = X.drop([drop_column], axis=1)\n",
    "        X_scaled =  preprocessing.scale(X, axis=1)\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=-2, init='k-means++').fit(X_scaled)\n",
    "        Y_pred = kmeans.labels_\n",
    "\n",
    "        conf_mat = confusion_matrix(Y_true, Y_pred)\n",
    "        accuracy = (conf_mat[0,0] + conf_mat[1,1]) / (conf_mat[0,0] + conf_mat[1,1] + conf_mat[0,1] + conf_mat[1,0])\n",
    "        silhouette = silhouette_score(X_scaled, Y_pred)\n",
    "        f1 = f1_score(Y_true, Y_pred)\n",
    "\n",
    "        \n",
    "        if ((silhouette > best_silhouette) and (f1 > best_f1)):\n",
    "            best_silhouette = silhouette\n",
    "            best_drop_order = to_drop\n",
    "            best_f1 = f1\n",
    "            \n",
    "            print(\"Silhouette score: \", silhouette)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "            print(\"f1: \", f1)\n",
    "\n",
    "```\n",
    "in order to find out that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_drop_order = ['victories',\n",
    " 'goals',\n",
    " 'defeats',\n",
    " 'redCards',\n",
    " 'day',\n",
    " 'month',\n",
    " 'meanExp',\n",
    " 'year',\n",
    " 'yellowCards',\n",
    " 'ties',\n",
    " 'yellowReds',\n",
    " 'height',\n",
    " 'nExp',\n",
    " 'nIAT',\n",
    " 'weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means only one feature is left out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_drop = ['day', 'month','height', 'weight', 'goals','nIAT', 'nExp', 'meanExp','ties', 'yellowReds', 'yellowCards', 'defeats', 'year', 'games', 'victories', 'redCards']\n",
    "left_feature = list(set(to_drop) - set(best_drop_order))\n",
    "left_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-cluster for this best drop order and print some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_copy\n",
    "silhouettes = []\n",
    "for drop_column in best_drop_order:\n",
    "    X = X.drop([drop_column], axis=1)\n",
    "    X_scaled =  preprocessing.scale(X, axis=1)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=-2, init='k-means++').fit(X_scaled)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    conf_mat = confusion_matrix(Y_true, Y_pred)\n",
    "    silhouette = silhouette_score(X_scaled, Y_pred)\n",
    "    silhouettes.append(silhouette)\n",
    "    \n",
    "plt.title('Silhouette score after each feature drop')\n",
    "silhouettes_plt = sns.barplot(x=best_drop_order, y=silhouettes, palette=\"Greens_d\")\n",
    "plt.setp(silhouettes_plt.get_xticklabels(), rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plot_helpers.custom_confusion_matrix(conf_mat, classes=['White skin', 'Black skin'],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print(\"Accuracy: \", (conf_mat[0,0] + conf_mat[1,1]) / (conf_mat[0,0] + conf_mat[1,1] + conf_mat[0,1] + conf_mat[1,0]))\n",
    "print(\"F1 score: \", f1_score(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low F1 score speaks for itself.\n",
    "This result upholds our previous thoughts and assumptions - there is no way of creating two distinct clusters where players with dark and light skin colors belong to different clusters. We can easily clusterize players such that silhouette score is high but this result is not skin color dependent. \n",
    "\n",
    "Ufff...! We are glad that when it comes to sports racism is not a thing! :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
